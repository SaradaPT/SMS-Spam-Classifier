
import numpy as np 
import pandas as pd
df = pd.read_csv('spam.csv', encoding='latin-1')
df.head(10)
v1	v2	Unnamed: 2	Unnamed: 3	Unnamed: 4
0	ham	Go until jurong point, crazy.. Available only ...	NaN	NaN	NaN
1	ham	Ok lar... Joking wif u oni...	NaN	NaN	NaN
2	spam	Free entry in 2 a wkly comp to win FA Cup fina...	NaN	NaN	NaN
3	ham	U dun say so early hor... U c already then say...	NaN	NaN	NaN
4	ham	Nah I don't think he goes to usf, he lives aro...	NaN	NaN	NaN
5	spam	FreeMsg Hey there darling it's been 3 week's n...	NaN	NaN	NaN
6	ham	Even my brother is not like to speak with me. ...	NaN	NaN	NaN
7	ham	As per your request 'Melle Melle (Oru Minnamin...	NaN	NaN	NaN
8	spam	WINNER!! As a valued network customer you have...	NaN	NaN	NaN
9	spam	Had your mobile 11 months or more? U R entitle...	NaN	NaN	NaN
df.shape
(5572, 5)
#1- Data Cleaning
#2- EDA
#3- Text Preprocessing 
#4- Model Building 
#5- Evalution
#6- Improvement 
#7 - Website 
1- Data Cleaning
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5572 entries, 0 to 5571
Data columns (total 5 columns):
 #   Column      Non-Null Count  Dtype 
---  ------      --------------  ----- 
 0   v1          5572 non-null   object
 1   v2          5572 non-null   object
 2   Unnamed: 2  50 non-null     object
 3   Unnamed: 3  12 non-null     object
 4   Unnamed: 4  6 non-null      object
dtypes: object(5)
memory usage: 217.8+ KB
#Here column 2 , 3 ,4  are of no use , so removing it

df.drop(columns = ['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)
df.sample(5)
v1	v2
2957	spam	U have a secret admirer. REVEAL who thinks U R...
732	ham	Lol you won't feel bad when I use her money to...
4729	ham	I dont know ask to my brother. Nothing problem...
264	ham	Why you Dint come with us.
3533	ham	Good evening! How are you?
#Renaming the columns

df.rename(columns={'v1':'target','v2':'text'},inplace=True)
df.head()
target	text
0	ham	Go until jurong point, crazy.. Available only ...
1	ham	Ok lar... Joking wif u oni...
2	spam	Free entry in 2 a wkly comp to win FA Cup fina...
3	ham	U dun say so early hor... U c already then say...
4	ham	Nah I don't think he goes to usf, he lives aro...
#Changing the target to 0 and 1

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
encoder.fit_transform(df['target'])
array([0, 0, 1, ..., 0, 0, 0])
df['target']=encoder.fit_transform(df['target'])
df.head()   #ham = 0 #spam = 1
target	text
0	0	Go until jurong point, crazy.. Available only ...
1	0	Ok lar... Joking wif u oni...
2	1	Free entry in 2 a wkly comp to win FA Cup fina...
3	0	U dun say so early hor... U c already then say...
4	0	Nah I don't think he goes to usf, he lives aro...
#Missing values 

df.isnull().sum()
target    0
text      0
dtype: int64
#Duplicated  values 

df.duplicated().sum()
403
#Drop duplicate values 

df = df.drop_duplicates(keep= 'first')
df.duplicated().sum()
0
2- EDA (EXPLORATORY DATA ANALYSIS)
#Check how many spam msgs are there 
df['target'].value_counts()
0    4516
1     653
Name: target, dtype: int64
import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(),labels=('ham','spam'),autopct='%0.2f')
plt.show()

#Data Is Imbalanced
import nltk
nltk.download('punkt')
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\sanya\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
True
#Counting the number of words

df['num_characters'] = df['text'].apply(len)
df.head()
target	text	num_characters
0	0	Go until jurong point, crazy.. Available only ...	111
1	0	Ok lar... Joking wif u oni...	29
2	1	Free entry in 2 a wkly comp to win FA Cup fina...	155
3	0	U dun say so early hor... U c already then say...	49
4	0	Nah I don't think he goes to usf, he lives aro...	61
#num  of words

df['num_words']=df['text'].apply(lambda x:len(nltk.word_tokenize(x)))
df.head()
target	text	num_characters	num_words
0	0	Go until jurong point, crazy.. Available only ...	111	24
1	0	Ok lar... Joking wif u oni...	29	8
2	1	Free entry in 2 a wkly comp to win FA Cup fina...	155	37
3	0	U dun say so early hor... U c already then say...	49	13
4	0	Nah I don't think he goes to usf, he lives aro...	61	15
#Num Of Sentences 

df['num_sentences']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))
df.head()
target	text	num_characters	num_words	num_sentences
0	0	Go until jurong point, crazy.. Available only ...	111	24	2
1	0	Ok lar... Joking wif u oni...	29	8	2
2	1	Free entry in 2 a wkly comp to win FA Cup fina...	155	37	2
3	0	U dun say so early hor... U c already then say...	49	13	1
4	0	Nah I don't think he goes to usf, he lives aro...	61	15	1
df[['num_characters','num_words','num_sentences']].describe()
num_characters	num_words	num_sentences
count	5169.000000	5169.000000	5169.000000
mean	78.977945	18.455794	1.965564
std	58.236293	13.324758	1.448541
min	2.000000	1.000000	1.000000
25%	36.000000	9.000000	1.000000
50%	60.000000	15.000000	1.000000
75%	117.000000	26.000000	2.000000
max	910.000000	220.000000	38.000000
#Only for HAM messages 

df[df['target']==0][['num_characters','num_words','num_sentences']].describe()
num_characters	num_words	num_sentences
count	4516.000000	4516.000000	4516.000000
mean	70.459256	17.123782	1.820195
std	56.358207	13.493970	1.383657
min	2.000000	1.000000	1.000000
25%	34.000000	8.000000	1.000000
50%	52.000000	13.000000	1.000000
75%	90.000000	22.000000	2.000000
max	910.000000	220.000000	38.000000
#Only For SPAM meassages

df[df['target']==1][['num_characters','num_words','num_sentences']].describe()
num_characters	num_words	num_sentences
count	653.000000	653.000000	653.000000
mean	137.891271	27.667688	2.970904
std	30.137753	7.008418	1.488425
min	13.000000	2.000000	1.000000
25%	132.000000	25.000000	2.000000
50%	149.000000	29.000000	3.000000
75%	157.000000	32.000000	4.000000
max	224.000000	46.000000	9.000000
import seaborn as sns
plt.figure(figsize=(12,6))
sns.histplot(df[df['target']==0]['num_characters'])
sns.histplot(df[df['target']==1]['num_characters'],color = 'red')
<Axes: xlabel='num_characters', ylabel='Count'>

Here we can see that spam messages has more number of words used than normal message
plt.figure(figsize=(12,6))
sns.histplot(df[df['target']==0]['num_words'])
sns.histplot(df[df['target']==1]['num_words'],color = 'red')
<Axes: xlabel='num_words', ylabel='Count'>

Here also we can observe the same thing that the spam messages are made of more number of words than normal message
sns.heatmap(df.corr(),annot = True)
C:\Users\sanya\AppData\Local\Temp\ipykernel_5660\2221401063.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.
  sns.heatmap(df.corr(),annot = True)
<Axes: >

Here in the heatmap we can see that num_characters, num_words, num_sentences are strongly correlated , so we cant keep three of them , now only taking num_characters ahead .
3 - DATA PREPROCESSING
Lower case.
Tokenization.
Removing Special characters.
Removing Stop words and punctuation .
Stemming
from nltk.corpus import stopwords
stopwords.words('english')
['i',
 'me',
 'my',
 'myself',
 'we',
 'our',
 'ours',
 'ourselves',
 'you',
 "you're",
 "you've",
 "you'll",
 "you'd",
 'your',
 'yours',
 'yourself',
 'yourselves',
 'he',
 'him',
 'his',
 'himself',
 'she',
 "she's",
 'her',
 'hers',
 'herself',
 'it',
 "it's",
 'its',
 'itself',
 'they',
 'them',
 'their',
 'theirs',
 'themselves',
 'what',
 'which',
 'who',
 'whom',
 'this',
 'that',
 "that'll",
 'these',
 'those',
 'am',
 'is',
 'are',
 'was',
 'were',
 'be',
 'been',
 'being',
 'have',
 'has',
 'had',
 'having',
 'do',
 'does',
 'did',
 'doing',
 'a',
 'an',
 'the',
 'and',
 'but',
 'if',
 'or',
 'because',
 'as',
 'until',
 'while',
 'of',
 'at',
 'by',
 'for',
 'with',
 'about',
 'against',
 'between',
 'into',
 'through',
 'during',
 'before',
 'after',
 'above',
 'below',
 'to',
 'from',
 'up',
 'down',
 'in',
 'out',
 'on',
 'off',
 'over',
 'under',
 'again',
 'further',
 'then',
 'once',
 'here',
 'there',
 'when',
 'where',
 'why',
 'how',
 'all',
 'any',
 'both',
 'each',
 'few',
 'more',
 'most',
 'other',
 'some',
 'such',
 'no',
 'nor',
 'not',
 'only',
 'own',
 'same',
 'so',
 'than',
 'too',
 'very',
 's',
 't',
 'can',
 'will',
 'just',
 'don',
 "don't",
 'should',
 "should've",
 'now',
 'd',
 'll',
 'm',
 'o',
 're',
 've',
 'y',
 'ain',
 'aren',
 "aren't",
 'couldn',
 "couldn't",
 'didn',
 "didn't",
 'doesn',
 "doesn't",
 'hadn',
 "hadn't",
 'hasn',
 "hasn't",
 'haven',
 "haven't",
 'isn',
 "isn't",
 'ma',
 'mightn',
 "mightn't",
 'mustn',
 "mustn't",
 'needn',
 "needn't",
 'shan',
 "shan't",
 'shouldn',
 "shouldn't",
 'wasn',
 "wasn't",
 'weren',
 "weren't",
 'won',
 "won't",
 'wouldn',
 "wouldn't"]
def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)
    
    #removing special characters 
    y = []
    for i in text:
        if i.isalnum():
            y.append(i)
            
    #removing stopwords and punctuations
    text = y[:]
    y.clear()
    for i in text:
        if i  not in stopwords.words('english') and  i not in string.punctuation:
            y.append(i)
            
    #Stemming 
    text = y[:]
    y.clear()
    
    for i in text:
            y.append(ps.stem(i))
        
    return ''.join(y)    
transform_text('Hi HOW ARE %% YOU ! Mousami , you were singing')
['hi', 'mousami', 'sing']
import string
string.punctuation
'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
ps.stem('Loving')
'love'
df['transformed_text']=df['text'].apply(transform_text)
df.head()
target	text	num_characters	num_words	num_sentences	transformed_text
0	0	Go until jurong point, crazy.. Available only ...	111	24	2	[go, jurong, point, crazi, avail, bugi, n, gre...
1	0	Ok lar... Joking wif u oni...	29	8	2	[ok, lar, joke, wif, u, oni]
2	1	Free entry in 2 a wkly comp to win FA Cup fina...	155	37	2	[free, entri, 2, wkli, comp, win, fa, cup, fin...
3	0	U dun say so early hor... U c already then say...	49	13	1	[u, dun, say, earli, hor, u, c, alreadi, say]
4	0	Nah I don't think he goes to usf, he lives aro...	61	15	1	[nah, think, goe, usf, live, around, though]
#let's make the world cloud

from wordcloud import WordCloud 

wc  = WordCloud(width =50,height =50,min_font_size=10,background_color='white')
#FOR SPAM MESSAGES

text_data = df[df['target'] == 1]['transformed_text'].astype(str).str.cat(sep='')

# Generate the word cloud
wc = WordCloud(width=800, height=400, background_color='white').generate(text_data)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

#FOR HEM MESSAGES

text_data = df[df['target'] == 0]['transformed_text'].astype(str).str.cat(sep='')

# Generate the word cloud
wc = WordCloud(width=800, height=400, background_color='white').generate(text_data)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

# To find the  words that are mostly used 

spam_corpus = []
for msg in df[df['target'] == 1]['transformed_text'].tolist():
    if isinstance(msg, str):  # Check if msg is a string
        for word in msg.split():
            spam_corpus.append(word)
    elif isinstance(msg, list):  # Check if msg is a list
        for sublist in msg:
            for word in sublist.split():
                spam_corpus.append(word)
            
len(spam_corpus)
9939
from collections import Counter
spam_counter = Counter(spam_corpus).most_common(30)
df_new = pd.DataFrame(spam_counter, columns=['Word', 'Count'])

sns.barplot(x='Word', y='Count', data=df_new)
plt.xticks(rotation ='vertical')
plt.show()

So , these are the top 30 words that are maximum used in spam messages .
4-Model Building
from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer
cv = CountVectorizer()
tfid = TfidfVectorizer()
df.columns
Index(['target', 'text', 'num_characters', 'num_words', 'num_sentences',
       'transformed_text'],
      dtype='object')
df['transformed_text'] = df['transformed_text'].astype(str)
X = cv.fit_transform(df['transformed_text']).toarray()
X
array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)
X.shape
(5169, 6708)
y = df['target'].values
y
array([0, 0, 1, ..., 0, 0, 0])
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score
gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()
gnb.fit(X_train,y_train)

y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))
0.8800773694390716
[[792 104]
 [ 20 118]]
0.5315315315315315
mnb.fit(X_train,y_train)

y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))
0.9642166344294004
[[871  25]
 [ 12 126]]
0.8344370860927153
bnb.fit(X_train,y_train)

y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))
0.9700193423597679
[[893   3]
 [ 28 110]]
0.9734513274336283
print('-------------------------------------------------------------------------------------------------------------')
-------------------------------------------------------------------------------------------------------------
df['transformed_text'] = df['transformed_text'].astype(str)
X = tfid.fit_transform(df['transformed_text']).toarray()
X
array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]])
X.shape
(5169, 6708)
y = df['target'].values
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)
gnb1 = GaussianNB()
mnb1 = MultinomialNB()
bnb1 = BernoulliNB()
gnb1.fit(X_train,y_train)

y_pred1 = gnb1.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))
0.8762088974854932
[[793 103]
 [ 25 113]]
0.5231481481481481
mnb1.fit(X_train,y_train)

y_pred2 = mnb1.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))
0.9593810444874274
[[896   0]
 [ 42  96]]
1.0
bnb1.fit(X_train,y_train)

y_pred3 = bnb1.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))
0.9700193423597679
[[893   3]
 [ 28 110]]
0.9734513274336283
So , the best model is - tfif -- MNB
Now , let's bring all the machine learning alogorithms and check the accuracy
 
Collecting xgboostNote: you may need to restart the kernel to use updated packages.

  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/24/ec/ad387100fa3cc2b9b81af0829b5ecfe75ec5bb19dd7c19d4fea06fb81802/xgboost-2.0.3-py3-none-win_amd64.whl.metadata
  Downloading xgboost-2.0.3-py3-none-win_amd64.whl.metadata (2.0 kB)
Requirement already satisfied: numpy in c:\users\sanya\anaconda3\lib\site-packages (from xgboost) (1.24.3)
Requirement already satisfied: scipy in c:\users\sanya\anaconda3\lib\site-packages (from xgboost) (1.10.1)
Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)
   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--
   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--
   ---------------------------------------- 0.2/99.8 MB 1.8 MB/s eta 0:00:55
   ---------------------------------------- 0.5/99.8 MB 3.5 MB/s eta 0:00:29
   ---------------------------------------- 0.8/99.8 MB 4.5 MB/s eta 0:00:23
    --------------------------------------- 1.4/99.8 MB 6.3 MB/s eta 0:00:16
    --------------------------------------- 2.2/99.8 MB 7.7 MB/s eta 0:00:13
   - -------------------------------------- 2.8/99.8 MB 8.6 MB/s eta 0:00:12
   - -------------------------------------- 3.5/99.8 MB 9.4 MB/s eta 0:00:11
   - -------------------------------------- 4.2/99.8 MB 9.9 MB/s eta 0:00:10
   - -------------------------------------- 4.9/99.8 MB 10.3 MB/s eta 0:00:10
   - -------------------------------------- 4.9/99.8 MB 10.3 MB/s eta 0:00:10
   - -------------------------------------- 4.9/99.8 MB 10.3 MB/s eta 0:00:10
   - -------------------------------------- 4.9/99.8 MB 10.3 MB/s eta 0:00:10
   -- ------------------------------------- 7.0/99.8 MB 10.8 MB/s eta 0:00:09
   --- ------------------------------------ 7.6/99.8 MB 10.8 MB/s eta 0:00:09
   --- ------------------------------------ 8.3/99.8 MB 11.0 MB/s eta 0:00:09
   --- ------------------------------------ 9.0/99.8 MB 11.3 MB/s eta 0:00:09
   --- ------------------------------------ 9.7/99.8 MB 11.4 MB/s eta 0:00:08
   ---- ----------------------------------- 10.5/99.8 MB 13.1 MB/s eta 0:00:07
   ---- ----------------------------------- 11.4/99.8 MB 14.2 MB/s eta 0:00:07
   ---- ----------------------------------- 12.1/99.8 MB 13.9 MB/s eta 0:00:07
   ----- ---------------------------------- 12.7/99.8 MB 13.9 MB/s eta 0:00:07
   ----- ---------------------------------- 13.4/99.8 MB 13.9 MB/s eta 0:00:07
   ----- ---------------------------------- 14.1/99.8 MB 14.2 MB/s eta 0:00:07
   ----- ---------------------------------- 14.8/99.8 MB 13.9 MB/s eta 0:00:07
   ------ --------------------------------- 15.5/99.8 MB 17.2 MB/s eta 0:00:05
   ------ --------------------------------- 16.1/99.8 MB 16.4 MB/s eta 0:00:06
   ------ --------------------------------- 16.9/99.8 MB 15.2 MB/s eta 0:00:06
   ------- -------------------------------- 17.9/99.8 MB 16.0 MB/s eta 0:00:06
   ------- -------------------------------- 18.9/99.8 MB 16.4 MB/s eta 0:00:05
   ------- -------------------------------- 19.9/99.8 MB 16.8 MB/s eta 0:00:05
   -------- ------------------------------- 20.8/99.8 MB 16.8 MB/s eta 0:00:05
   -------- ------------------------------- 21.5/99.8 MB 16.4 MB/s eta 0:00:05
   -------- ------------------------------- 22.2/99.8 MB 16.8 MB/s eta 0:00:05
   --------- ------------------------------ 22.9/99.8 MB 16.4 MB/s eta 0:00:05
   --------- ------------------------------ 23.6/99.8 MB 16.8 MB/s eta 0:00:05
   --------- ------------------------------ 24.4/99.8 MB 16.4 MB/s eta 0:00:05
   ---------- ----------------------------- 25.0/99.8 MB 16.4 MB/s eta 0:00:05
   ---------- ----------------------------- 25.7/99.8 MB 16.4 MB/s eta 0:00:05
   ---------- ----------------------------- 26.4/99.8 MB 16.4 MB/s eta 0:00:05
   ---------- ----------------------------- 27.1/99.8 MB 16.4 MB/s eta 0:00:05
   ----------- ---------------------------- 27.8/99.8 MB 16.0 MB/s eta 0:00:05
   ----------- ---------------------------- 28.5/99.8 MB 15.6 MB/s eta 0:00:05
   ----------- ---------------------------- 29.2/99.8 MB 15.2 MB/s eta 0:00:05
   ----------- ---------------------------- 29.8/99.8 MB 14.9 MB/s eta 0:00:05
   ------------ --------------------------- 30.5/99.8 MB 14.9 MB/s eta 0:00:05
   ------------ --------------------------- 31.2/99.8 MB 14.6 MB/s eta 0:00:05
   ------------ --------------------------- 31.8/99.8 MB 14.5 MB/s eta 0:00:05
   ------------ --------------------------- 32.1/99.8 MB 14.6 MB/s eta 0:00:05
   ------------ --------------------------- 32.1/99.8 MB 14.6 MB/s eta 0:00:05
   ------------- -------------------------- 33.8/99.8 MB 14.2 MB/s eta 0:00:05
   ------------- -------------------------- 34.4/99.8 MB 14.2 MB/s eta 0:00:05
   -------------- ------------------------- 35.3/99.8 MB 14.6 MB/s eta 0:00:05
   -------------- ------------------------- 36.1/99.8 MB 14.9 MB/s eta 0:00:05
   -------------- ------------------------- 36.8/99.8 MB 14.6 MB/s eta 0:00:05
   --------------- ------------------------ 37.5/99.8 MB 14.9 MB/s eta 0:00:05
   --------------- ------------------------ 38.2/99.8 MB 14.6 MB/s eta 0:00:05
   --------------- ------------------------ 38.8/99.8 MB 14.6 MB/s eta 0:00:05
   --------------- ------------------------ 39.6/99.8 MB 14.9 MB/s eta 0:00:05
   ---------------- ----------------------- 40.3/99.8 MB 14.9 MB/s eta 0:00:04
   ---------------- ----------------------- 41.2/99.8 MB 15.2 MB/s eta 0:00:04
   ---------------- ----------------------- 42.0/99.8 MB 15.6 MB/s eta 0:00:04
   ----------------- ---------------------- 42.6/99.8 MB 17.7 MB/s eta 0:00:04
   ----------------- ---------------------- 43.3/99.8 MB 16.4 MB/s eta 0:00:04
   ----------------- ---------------------- 44.0/99.8 MB 15.6 MB/s eta 0:00:04
   ----------------- ---------------------- 44.7/99.8 MB 15.2 MB/s eta 0:00:04
   ------------------ --------------------- 45.5/99.8 MB 15.2 MB/s eta 0:00:04
   ------------------ --------------------- 46.2/99.8 MB 14.9 MB/s eta 0:00:04
   ------------------ --------------------- 46.9/99.8 MB 14.9 MB/s eta 0:00:04
   ------------------- -------------------- 47.6/99.8 MB 15.2 MB/s eta 0:00:04
   ------------------- -------------------- 48.4/99.8 MB 15.2 MB/s eta 0:00:04
   ------------------- -------------------- 49.0/99.8 MB 15.2 MB/s eta 0:00:04
   ------------------- -------------------- 49.8/99.8 MB 15.2 MB/s eta 0:00:04
   -------------------- ------------------- 50.5/99.8 MB 15.2 MB/s eta 0:00:04
   -------------------- ------------------- 51.2/99.8 MB 15.2 MB/s eta 0:00:04
   -------------------- ------------------- 51.9/99.8 MB 14.9 MB/s eta 0:00:04
   --------------------- ------------------ 52.6/99.8 MB 14.9 MB/s eta 0:00:04
   --------------------- ------------------ 53.4/99.8 MB 15.2 MB/s eta 0:00:04
   --------------------- ------------------ 54.1/99.8 MB 15.2 MB/s eta 0:00:03
   --------------------- ------------------ 54.8/99.8 MB 15.2 MB/s eta 0:00:03
   ---------------------- ----------------- 55.5/99.8 MB 15.2 MB/s eta 0:00:03
   ---------------------- ----------------- 56.5/99.8 MB 15.6 MB/s eta 0:00:03
   ----------------------- ---------------- 57.6/99.8 MB 16.0 MB/s eta 0:00:03
   ----------------------- ---------------- 58.2/99.8 MB 16.4 MB/s eta 0:00:03
   ----------------------- ---------------- 59.0/99.8 MB 16.0 MB/s eta 0:00:03
   ----------------------- ---------------- 59.7/99.8 MB 16.4 MB/s eta 0:00:03
   ------------------------ --------------- 60.4/99.8 MB 16.0 MB/s eta 0:00:03
   ------------------------ --------------- 61.1/99.8 MB 16.0 MB/s eta 0:00:03
   ------------------------ --------------- 61.8/99.8 MB 16.0 MB/s eta 0:00:03
   ------------------------- -------------- 62.5/99.8 MB 16.0 MB/s eta 0:00:03
   ------------------------- -------------- 63.2/99.8 MB 16.0 MB/s eta 0:00:03
   ------------------------- -------------- 63.9/99.8 MB 16.0 MB/s eta 0:00:03
   ------------------------- -------------- 64.7/99.8 MB 16.4 MB/s eta 0:00:03
   -------------------------- ------------- 65.5/99.8 MB 16.4 MB/s eta 0:00:03
   -------------------------- ------------- 66.2/99.8 MB 16.0 MB/s eta 0:00:03
   -------------------------- ------------- 66.8/99.8 MB 15.6 MB/s eta 0:00:03
   --------------------------- ------------ 67.6/99.8 MB 15.2 MB/s eta 0:00:03
   --------------------------- ------------ 68.4/99.8 MB 15.2 MB/s eta 0:00:03
   --------------------------- ------------ 69.3/99.8 MB 16.0 MB/s eta 0:00:02
   ---------------------------- ----------- 70.1/99.8 MB 16.0 MB/s eta 0:00:02
   ---------------------------- ----------- 70.8/99.8 MB 15.6 MB/s eta 0:00:02
   ---------------------------- ----------- 71.5/99.8 MB 15.6 MB/s eta 0:00:02
   ---------------------------- ----------- 72.2/99.8 MB 15.6 MB/s eta 0:00:02
   ----------------------------- ---------- 73.0/99.8 MB 15.6 MB/s eta 0:00:02
   ----------------------------- ---------- 73.7/99.8 MB 15.6 MB/s eta 0:00:02
   ----------------------------- ---------- 74.4/99.8 MB 15.6 MB/s eta 0:00:02
   ------------------------------ --------- 75.2/99.8 MB 15.6 MB/s eta 0:00:02
   ------------------------------ --------- 76.0/99.8 MB 16.0 MB/s eta 0:00:02
   ------------------------------ --------- 76.8/99.8 MB 16.0 MB/s eta 0:00:02
   ------------------------------- -------- 77.6/99.8 MB 16.4 MB/s eta 0:00:02
   ------------------------------- -------- 78.5/99.8 MB 16.4 MB/s eta 0:00:02
   ------------------------------- -------- 79.3/99.8 MB 16.4 MB/s eta 0:00:02
   -------------------------------- ------- 80.1/99.8 MB 16.4 MB/s eta 0:00:02
   -------------------------------- ------- 80.9/99.8 MB 16.4 MB/s eta 0:00:02
   -------------------------------- ------- 81.8/99.8 MB 16.8 MB/s eta 0:00:02
   --------------------------------- ------ 82.6/99.8 MB 16.8 MB/s eta 0:00:02
   --------------------------------- ------ 83.6/99.8 MB 17.2 MB/s eta 0:00:01
   --------------------------------- ------ 84.5/99.8 MB 17.7 MB/s eta 0:00:01
   ---------------------------------- ----- 85.3/99.8 MB 17.7 MB/s eta 0:00:01
   ---------------------------------- ----- 86.2/99.8 MB 18.2 MB/s eta 0:00:01
   ---------------------------------- ----- 87.0/99.8 MB 18.2 MB/s eta 0:00:01
   ----------------------------------- ---- 87.8/99.8 MB 18.2 MB/s eta 0:00:01
   ----------------------------------- ---- 88.7/99.8 MB 18.2 MB/s eta 0:00:01
   ----------------------------------- ---- 89.4/99.8 MB 18.2 MB/s eta 0:00:01
   ------------------------------------ --- 90.2/99.8 MB 17.7 MB/s eta 0:00:01
   ------------------------------------ --- 91.0/99.8 MB 17.7 MB/s eta 0:00:01
   ------------------------------------ --- 91.7/99.8 MB 17.2 MB/s eta 0:00:01
   ------------------------------------- -- 92.5/99.8 MB 17.7 MB/s eta 0:00:01
   ------------------------------------- -- 93.3/99.8 MB 17.7 MB/s eta 0:00:01
   ------------------------------------- -- 94.1/99.8 MB 16.8 MB/s eta 0:00:01
   -------------------------------------- - 94.9/99.8 MB 16.8 MB/s eta 0:00:01
   -------------------------------------- - 95.6/99.8 MB 16.8 MB/s eta 0:00:01
   -------------------------------------- - 96.4/99.8 MB 16.4 MB/s eta 0:00:01
   -------------------------------------- - 97.1/99.8 MB 16.0 MB/s eta 0:00:01
   ---------------------------------------  97.8/99.8 MB 16.4 MB/s eta 0:00:01
   ---------------------------------------  98.6/99.8 MB 16.0 MB/s eta 0:00:01
   ---------------------------------------  99.3/99.8 MB 16.0 MB/s eta 0:00:01
   ---------------------------------------  99.7/99.8 MB 15.6 MB/s eta 0:00:01
   ---------------------------------------  99.7/99.8 MB 15.6 MB/s eta 0:00:01
   ---------------------------------------  99.7/99.8 MB 15.6 MB/s eta 0:00:01
   ---------------------------------------  99.7/99.8 MB 15.6 MB/s eta 0:00:01
   ---------------------------------------  99.7/99.8 MB 15.6 MB/s eta 0:00:01
   ---------------------------------------- 99.8/99.8 MB 10.9 MB/s eta 0:00:00
Installing collected packages: xgboost
Successfully installed xgboost-2.0.3
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier 
svc = SVC(kernel='sigmoid',gamma=1.0)
knc =KNeighborsClassifier()
mnb = MultinomialNB()
dtc = DecisionTreeClassifier(max_depth=5)
lrc = LogisticRegression(solver='liblinear')
rfc =RandomForestClassifier(n_estimators=50,random_state=2)
abc =AdaBoostClassifier(n_estimators=50,random_state=2)
bc = BaggingClassifier(n_estimators=50,random_state=2)
etc = ExtraTreesClassifier(n_estimators=50,random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)
xgb = XGBClassifier(n_estimators=50,random_state=2)
clfs = {
    'SVC' : svc,
    'KNN':knc,
    'Multinomial':mnb,
    'DT': dtc,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'BC':bc,
    'ExtraTree':etc,
    'GB': gbdt,
    'XGB': xgb
   
}
def train_classifier(clf,X_train,y_train,X_test,y_test):
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test)
    accuracy =accuracy_score(y_test,y_pred)
    precision = precision_score(y_test,y_pred)
    
    return accuracy,precision
train_classifier(svc,X_train,y_train,X_test,y_test)
(0.9729206963249516, 0.9741379310344828)
### Making of loop for evaluating all the algorithms ans storing the accuracy score and precision score 


from sklearn.metrics import accuracy_score, precision_score

accuracy_scores = []
precision_scores = []

for name, clf in clfs.items():
    current_accuracy, current_precision = train_classifier(clf, X_train, y_train, X_test, y_test)
    
    
    
    print('For', name)
    print('Accuracy - ', current_accuracy)
    print('Precision - ', current_precision)
    
    # Append accuracy and precision scores to the lists
    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)

        
For SVC
Accuracy -  0.9729206963249516
Precision -  0.9741379310344828
For KNN
Accuracy -  0.9003868471953579
Precision -  1.0
For Multinomial
Accuracy -  0.9593810444874274
Precision -  1.0
For DT
Accuracy -  0.9361702127659575
Precision -  0.8461538461538461
For LR
Accuracy -  0.9477756286266924
Precision -  0.9883720930232558
For RF
Accuracy -  0.971953578336557
Precision -  1.0
For AdaBoost
Accuracy -  0.9613152804642167
Precision -  0.9454545454545454
For BC
Accuracy -  0.9584139264990329
Precision -  0.8625954198473282
For ExtraTree
Accuracy -  0.9729206963249516
Precision -  0.9824561403508771
For GB
Accuracy -  0.9526112185686654
Precision -  0.9238095238095239
For XGB
Accuracy -  0.9729206963249516
Precision -  0.9435483870967742
import pandas as pd

# Assuming clfs, accuracy_scores, and precision_scores are lists
performance_df = pd.DataFrame({
    'Algorithm': list(clfs.keys()),
    'Accuracy': accuracy_scores,
    'Precision': precision_scores
}).sort_values('Precision', ascending=False)
performance_df
Algorithm	Accuracy	Precision
1	KNN	0.900387	1.000000
2	Multinomial	0.959381	1.000000
5	RF	0.971954	1.000000
4	LR	0.947776	0.988372
8	ExtraTree	0.972921	0.982456
0	SVC	0.972921	0.974138
6	AdaBoost	0.961315	0.945455
10	XGB	0.972921	0.943548
9	GB	0.952611	0.923810
7	BC	0.958414	0.862595
3	DT	0.936170	0.846154
Here after examing all the algorithms we came to know that Multinomial giving us the the accuracy and precision score . Now deploying the model
import pickle
pickle.dump(tfid,open('Vectorizer.pkl','wb'))
pickle.dump(mnb1,open('model.pkl','wb'))
 
